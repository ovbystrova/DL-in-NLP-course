{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "task2_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aft4QRoRoI3",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 1.2: Word2vec preprocessing (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVS7HanORoI-",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing is not the most exciting part of NLP, but it is still one of the most important ones. Your task is to preprocess raw text (you can use your own, or [this one](http://mattmahoney.net/dc/text8.zip). For this task text preprocessing mostly consists of:\n",
        "\n",
        "1. cleaning (mostly, if your dataset is from social media or parsed from the internet)\n",
        "1. tokenization\n",
        "1. building the vocabulary and choosing its size. Use only high-frequency words, change all other words to UNK or handle it in your own manner. You can use `collections.Counter` for that.\n",
        "1. assigning each token a number (numericalization). In other words, make word2index и index2word objects.\n",
        "1. data structuring and batching - make X and y matrices generator for word2vec (explained in more details below)\n",
        "\n",
        "**ATTN!:** If you use your own data, please, attach a download link. \n",
        "\n",
        "Your goal is to make **Batcher** class which returns two numpy tensors with word indices. It should be possible to use one for word2vec training. You can implement batcher for Skip-Gram or CBOW architecture, the picture below can be helpful to remember the difference.\n",
        "\n",
        "![text](https://raw.githubusercontent.com/deepmipt/deep-nlp-seminars/651804899d05b96fc72b9474404fab330365ca09/seminar_02/pics/architecture.png)\n",
        "\n",
        "There are several ways to do it right. Shapes could be `x_batch.shape = (batch_size, 2*window_size)`, `y_batch.shape = (batch_size,)` for CBOW or `(batch_size,)`, `(batch_size, 2*window_size)` for Skip-Gram. You should **not** do negative sampling here.\n",
        "\n",
        "They should be adequately parametrized: CBOW(window_size, ...), SkipGram(window_size, ...). You should implement only one batcher in this task; and it's up to you which one to chose.\n",
        "\n",
        "Useful links:\n",
        "1. [Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
        "1. [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
        "1. [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
        "\n",
        "You can write the code in this notebook, or in a separate file. It can be reused for the next task. The result of your work should represent that your batch has a proper structure (right shapes) and content (words should be from one context, not some random indices). To show that, translate indices back to words and print them to show something like this:\n",
        "\n",
        "```\n",
        "text = ['first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including']\n",
        "\n",
        "window_size = 2\n",
        "\n",
        "# CBOW:\n",
        "indices_to_words(x_batch) = \\\n",
        "        [['first', 'used', 'early', 'working'],\n",
        "        ['used', 'against', 'working', 'class'],\n",
        "        ['against', 'early', 'class', 'radicals'],\n",
        "        ['early', 'working', 'radicals', 'including']]\n",
        "\n",
        "indices_to_words(labels_batch) = ['against', 'early', 'working', 'class']\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl4MafhCsxxv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import numpy as np\n",
        "import spacy\n",
        "from spacy.symbols import ORTH\n",
        "\n",
        "spacy_en = spacy.load('en')\n",
        "spacy_en.tokenizer.add_special_case(\"don't\", [{ORTH: \"do\"}, {ORTH: \"not\"}])\n",
        "spacy_en.tokenizer.add_special_case(\"didn't\", [{ORTH: \"did\"}, {ORTH: \"not\"}]) #adding special case so that tokenizer(\"\"\"don't\"\"\") != 'do'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdqhXiINRoJC",
        "colab_type": "code",
        "outputId": "544d3fda-ba77-4c97-94c2-5baf12ffbe47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "!wget http://mattmahoney.net/dc/text8.zip\n",
        "!unzip text8.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-24 20:36:15--  http://mattmahoney.net/dc/text8.zip\n",
            "Resolving mattmahoney.net (mattmahoney.net)... 67.195.197.75\n",
            "Connecting to mattmahoney.net (mattmahoney.net)|67.195.197.75|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31344016 (30M) [application/zip]\n",
            "Saving to: ‘text8.zip’\n",
            "\n",
            "text8.zip           100%[===================>]  29.89M   288KB/s    in 1m 55s  \n",
            "\n",
            "2020-02-24 20:38:16 (267 KB/s) - ‘text8.zip’ saved [31344016/31344016]\n",
            "\n",
            "Archive:  text8.zip\n",
            "  inflating: text8                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg3RPUf7q9k3",
        "colab_type": "text"
      },
      "source": [
        "# The main part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvrvVADZtXLN",
        "colab_type": "code",
        "outputId": "85ebffcd-5148-4e3b-d388-f22a3682a84c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Opening data\n",
        "with open('text8', encoding='utf-8') as f:\n",
        "    text_original = f.read()\n",
        "print(text_original[:100])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " anarchism originated as a term of abuse first used against early working class radicals including t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnKOp7IAsjKY",
        "colab_type": "code",
        "outputId": "b3b616b6-d685-4407-b7d1-a03b51e8ff4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Preprocessing stuff\n",
        "def tokenizer(text):\n",
        "    \"\"\"\n",
        "    return: list of lemmas (without punctuation and numbers)\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text) if tok.text.isalpha()]\n",
        "\n",
        "tokens = tokenizer(text_original)\n",
        "print(len(tokens), len(set(tokens)))\n",
        "print(tokens[:10])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17008373 253830\n",
            "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6AsEhiBugjN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unk_token = '<unk>'\n",
        "pad_token = '<pad>'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lf3zPmPq_wL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Batcher(Dataset):\n",
        "    \"\"\"\n",
        "    Preprocessed list of tokens passed  here\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tokens, vocab_size):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.tokens = tokens\n",
        "        self.tokens_freq = []\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.word2index = {}\n",
        "        self.index2word = {}\n",
        "\n",
        "        print('Initial length of tokens: {}'.format(self.vocab_size))\n",
        "\n",
        "        self.build_vocab(min_freq=5)\n",
        "        self.numericalization()\n",
        "        self.x, self.y = self.cbow_batching(batch_size=64, window_size=4)\n",
        "\n",
        "    def __len__(self):        \n",
        "        return self.x.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        x = self.x[idx]\n",
        "        #x = torch.FloatTensor(x) # преобразуем в тензор с флоат величинами\n",
        "        y = self.y[idx]        \n",
        "        return x, y\n",
        "    \n",
        "    \n",
        "    def build_vocab(self, min_freq = 10):\n",
        "        \"\"\"\n",
        "        builds vocab (self.tokens_freq) from self.tokens\n",
        "        param: min_freq (int) - minimum frequency for token in list to get to vocab\n",
        "        \"\"\"\n",
        "        counter = Counter(self.tokens)\n",
        "        mask = list(map(lambda x: x[1] > min_freq, counter.items()))\n",
        "        self.tokens_freq = np.array(list(counter.items()))[mask]\n",
        "        self.tokens_freq = list(map(lambda x: x[0], self.tokens_freq)) + [unk_token] + [pad_token]\n",
        "        self.vocab_size = len(self.tokens_freq)\n",
        "\n",
        "        print('After building vocab, vocab_size: {}'.format(self.vocab_size))\n",
        "\n",
        "    def numericalization(self):\n",
        "        \"\"\"\n",
        "        creates word2index and index2word, replaces not frequent tokens with 'unk' token\n",
        "        \"\"\"\n",
        "        self.word2index = {word : ind for ind, word in enumerate(self.tokens_freq)}\n",
        "        self.index2word = {value : key for key, value in self.word2index.items()}\n",
        "\n",
        "\n",
        "\n",
        "        self.tokens = [self.word2index[token] if token in self.word2index else self.word2index[unk_token] \n",
        "                       for token in self.tokens]\n",
        "\n",
        "        print('Numeralization done. Example of self.tokens: {}'.format(self.tokens[:10]))          \n",
        "        \n",
        "\n",
        "    def cbow_batching(self, batch_size, window_size):\n",
        "        \"\"\"\n",
        "        adds pad_token, creates batches\n",
        "        \"\"\"\n",
        "        \n",
        "        self.tokens = [self.word2index[pad_token]] * window_size + self.tokens + [self.word2index[pad_token]] * window_size\n",
        "        x_batches = []\n",
        "        y_batches = []\n",
        "\n",
        "        for i in np.arange(window_size, len(self.tokens)-window_size):\n",
        "            y_batches.append(self.tokens[i])\n",
        "\n",
        "            context = self.tokens[i-window_size:i] + self.tokens[i+1:i+1+window_size]\n",
        "            x_batches.append(context)\n",
        "        x_batches = np.array(x_batches)\n",
        "        y_batches = np.array(y_batches)\n",
        "\n",
        "        try:\n",
        "            x_batches = x_batches.reshape((-1, batch_size,2*window_size))\n",
        "            y_batches = y_batches.reshape((-1,batch_size))\n",
        "        except Exception:\n",
        "            print('Could not reshape directly so deleted something')\n",
        "            total = len(y_batches)\n",
        "            x_batches = x_batches[:-(total % batch_size),:]\n",
        "            y_batches = y_batches[:-(total % batch_size)]\n",
        "\n",
        "            x_batches = x_batches.reshape((-1, batch_size,2*window_size))\n",
        "            y_batches = y_batches.reshape((-1,batch_size))\n",
        "\n",
        "        return x_batches, y_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxlhZ5QpShis",
        "colab_type": "code",
        "outputId": "8a875114-f378-4d39-a753-b65335e54ecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "batcher=Batcher(tokens=tokens, vocab_size=len(tokens))\n",
        "#batcher.build_vocab(min_freq=5)\n",
        "#batcher.numericalization()\n",
        "#x, y = batcher.cbow_batching(batch_size=64, window_size=4)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial length of tokens: 17008373\n",
            "After building vocab, vocab_size: 63632\n",
            "Numeralization done. Example of self.tokens: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Could not reshape directly so deleted something\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1DhuHw_tUbB",
        "colab_type": "code",
        "outputId": "e49f42d1-f5e8-452a-8bcc-6b21622f2457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "batcher.x.shape, batcher.y.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((265755, 64, 8), (265755, 64))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}