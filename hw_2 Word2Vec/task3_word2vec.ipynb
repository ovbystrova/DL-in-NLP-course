{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "task3_word2vec.ipynb",
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "117125abe5e34fac9ce47eea7de86ccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6906e05a5bb544fcb3c2fb340d58f831",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_01f6e53e6f2e405fa8568b91a619bd24",
              "IPY_MODEL_880eca7a029845759da84cd931ec8300"
            ]
          }
        },
        "6906e05a5bb544fcb3c2fb340d58f831": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "01f6e53e6f2e405fa8568b91a619bd24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8fc46a39fc5a485fac430715b2194f91",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "",
            "max": 212604,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_efe25e67c55a49d8b2eacd29f0350727"
          }
        },
        "880eca7a029845759da84cd931ec8300": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_da0b353a3d4a42098720c0e29235c60a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "  0% 0/212604 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_22c31003a709484eaa1392d3f8bfcfbd"
          }
        },
        "8fc46a39fc5a485fac430715b2194f91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "efe25e67c55a49d8b2eacd29f0350727": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "da0b353a3d4a42098720c0e29235c60a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "22c31003a709484eaa1392d3f8bfcfbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk50N_j-0Bib",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 1.3: Naive word2vec (40 points)\n",
        "\n",
        "This task can be formulated very simply. Follow this [paper](https://arxiv.org/pdf/1411.2738.pdf) and implement word2vec like a two-layer neural network with matrices $W$ and $W'$. One matrix projects words to low-dimensional 'hidden' space and the other - back to high-dimensional vocabulary space.\n",
        "\n",
        "![word2vec](https://i.stack.imgur.com/6eVXZ.jpg)\n",
        "\n",
        "You can use TensorFlow/PyTorch and code from your previous task.\n",
        "\n",
        "## Results of this task: (30 points)\n",
        " * trained word vectors (mention somewhere, how long it took to train)\n",
        " * plotted loss (so we can see that it has converged)\n",
        " * function to map token to corresponding word vector\n",
        " * beautiful visualizations (PCE, T-SNE), you can use TensorBoard and play with your vectors in 3D (don't forget to add screenshots to the task)\n",
        "\n",
        "## Extra questions: (10 points)\n",
        " * Intrinsic evaluation: you can find datasets [here](http://download.tensorflow.org/data/questions-words.txt)\n",
        " * Extrinsic evaluation: you can use [these](https://medium.com/@dataturks/rare-text-classification-open-datasets-9d340c8c508e)\n",
        "\n",
        "Also, you can find any other datasets for quantitative evaluation.\n",
        "\n",
        "Again. It is **highly recommended** to read this [paper](https://arxiv.org/pdf/1411.2738.pdf)\n",
        "\n",
        "Example of visualization in tensorboard:\n",
        "https://projector.tensorflow.org\n",
        "\n",
        "Example of 2D visualisation:\n",
        "\n",
        "![2dword2vec](https://www.tensorflow.org/images/tsne.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atjyQgFiR9vq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO CrossEntropy пофиксить\n",
        "# Допилить лосс функцию.\n",
        "# Сделать функцию, которая вытаскивает вектор по нидексу слова."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7M1xy1gxdOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "import numpy as np\n",
        "import spacy\n",
        "from spacy.symbols import ORTH\n",
        "\n",
        "spacy_en = spacy.load('en')\n",
        "spacy_en.tokenizer.add_special_case(\"don't\", [{ORTH: \"do\"}, {ORTH: \"not\"}])\n",
        "spacy_en.tokenizer.add_special_case(\"didn't\", [{ORTH: \"did\"}, {ORTH: \"not\"}]) #adding special case so that tokenizer(\"\"\"don't\"\"\") != 'do'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WTaVuP4xfu2",
        "colab_type": "code",
        "outputId": "047c19c5-25be-46d4-cba6-a128d9ab4c32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "!wget http://mattmahoney.net/dc/text8.zip\n",
        "!unzip text8.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-24 20:50:03--  http://mattmahoney.net/dc/text8.zip\n",
            "Resolving mattmahoney.net (mattmahoney.net)... 67.195.197.75\n",
            "Connecting to mattmahoney.net (mattmahoney.net)|67.195.197.75|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31344016 (30M) [application/zip]\n",
            "Saving to: ‘text8.zip’\n",
            "\n",
            "text8.zip           100%[===================>]  29.89M  2.16MB/s    in 14s     \n",
            "\n",
            "2020-02-24 20:50:18 (2.13 MB/s) - ‘text8.zip’ saved [31344016/31344016]\n",
            "\n",
            "Archive:  text8.zip\n",
            "  inflating: text8                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7T6uhoWxqtB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unk_token = '<unk>'\n",
        "pad_token = '<pad>'\n",
        "BATCH_SIZE = 64\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifYz7tlpw2s1",
        "colab_type": "text"
      },
      "source": [
        "# Batch and Loader stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHrYbwGLw4dD",
        "colab_type": "code",
        "outputId": "5c0a1391-3ce6-497b-fc71-60c6bf813ca2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Opening data\n",
        "with open('text8', encoding='utf-8') as f:\n",
        "    text_original = f.read()\n",
        "print(text_original[:100])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " anarchism originated as a term of abuse first used against early working class radicals including t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaveXQfWxnIC",
        "colab_type": "code",
        "outputId": "d82707da-6835-4af2-a6b2-4e8d7beb54f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Preprocessing stuff\n",
        "def tokenizer(text):\n",
        "    \"\"\"\n",
        "    return: list of lemmas (without punctuation and numbers)\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text) if tok.text.isalpha()]\n",
        "\n",
        "tokens = tokenizer(text_original)\n",
        "print(len(tokens), len(set(tokens)))\n",
        "print(tokens[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17008373 253830\n",
            "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Mpx7ESdxu3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Batcher(Dataset):\n",
        "    \"\"\"\n",
        "    Preprocessed list of tokens passed  here\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tokens, vocab_size):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.tokens = tokens\n",
        "        self.tokens_freq = []\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.word2index = {}\n",
        "        self.index2word = {}\n",
        "\n",
        "        print('Initial length of tokens: {}'.format(self.vocab_size))\n",
        "\n",
        "        self.build_vocab(min_freq=5)\n",
        "        self.numericalization()\n",
        "        self.x, self.y = self.cbow_batching(batch_size=64, window_size=4)\n",
        "\n",
        "    def __len__(self):        \n",
        "        return self.x.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        x = self.x[idx]\n",
        "        #x = torch.FloatTensor(x) # преобразуем в тензор с флоат величинами\n",
        "        y = self.y[idx]        \n",
        "        return x, y\n",
        "    \n",
        "    \n",
        "    def build_vocab(self, min_freq = 10):\n",
        "        \"\"\"\n",
        "        builds vocab (self.tokens_freq) from self.tokens\n",
        "        param: min_freq (int) - minimum frequency for token in list to get to vocab\n",
        "        \"\"\"\n",
        "        counter = Counter(self.tokens)\n",
        "        mask = list(map(lambda x: x[1] > min_freq, counter.items()))\n",
        "        self.tokens_freq = np.array(list(counter.items()))[mask]\n",
        "        self.tokens_freq = list(map(lambda x: x[0], self.tokens_freq)) + [unk_token] + [pad_token]\n",
        "        self.vocab_size = len(self.tokens_freq)\n",
        "\n",
        "        print('After building vocab, vocab_size: {}'.format(self.vocab_size))\n",
        "\n",
        "    def numericalization(self):\n",
        "        \"\"\"\n",
        "        creates word2index and index2word, replaces not frequent tokens with 'unk' token\n",
        "        \"\"\"\n",
        "        self.word2index = {word : ind for ind, word in enumerate(self.tokens_freq)}\n",
        "        self.index2word = {value : key for key, value in self.word2index.items()}\n",
        "\n",
        "\n",
        "\n",
        "        self.tokens = [self.word2index[token] if token in self.word2index else self.word2index[unk_token] \n",
        "                       for token in self.tokens]\n",
        "\n",
        "        print('Numeralization done. Example of self.tokens: {}'.format(self.tokens[:10]))          \n",
        "        \n",
        "\n",
        "    def cbow_batching(self, batch_size, window_size):\n",
        "        \"\"\"\n",
        "        adds pad_token, creates batches\n",
        "        \"\"\"\n",
        "        \n",
        "        self.tokens = [self.word2index[pad_token]] * window_size + self.tokens + [self.word2index[pad_token]] * window_size\n",
        "        x_batches = []\n",
        "        y_batches = []\n",
        "\n",
        "        for i in np.arange(window_size, len(self.tokens)-window_size):\n",
        "            y_batches.append(self.tokens[i])\n",
        "\n",
        "            context = self.tokens[i-window_size:i] + self.tokens[i+1:i+1+window_size]\n",
        "            x_batches.append(context)\n",
        "        x_batches = np.array(x_batches)\n",
        "        y_batches = np.array(y_batches)\n",
        "\n",
        "        try:\n",
        "            x_batches = x_batches.reshape((-1, batch_size,2*window_size))\n",
        "            y_batches = y_batches.reshape((-1,batch_size))\n",
        "        except Exception:\n",
        "            print('Could not reshape directly so deleted something')\n",
        "            total = len(y_batches)\n",
        "            x_batches = x_batches[:-(total % batch_size),:]\n",
        "            y_batches = y_batches[:-(total % batch_size)]\n",
        "\n",
        "            x_batches = x_batches.reshape((-1, batch_size,2*window_size))\n",
        "            y_batches = y_batches.reshape((-1,batch_size))\n",
        "\n",
        "        return x_batches, y_batches\n",
        "\n",
        "    def to_pytorch(self, x_batches, y_batches):\n",
        "        x_batches = torch.from_numpy(x_batches)\n",
        "        y_batches = torch.from_numpy(y_batches)\n",
        "        return x_batches, y_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKrK2b08xvlO",
        "colab_type": "code",
        "outputId": "739f1584-0883-4c2c-c2ba-cf624ecac8f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "batcher=Batcher(tokens=tokens, vocab_size=len(tokens))\n",
        "batcher.x, batcher.y = batcher.to_pytorch(batcher.x, batcher.y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial length of tokens: 17008373\n",
            "After building vocab, vocab_size: 63632\n",
            "Numeralization done. Example of self.tokens: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Could not reshape directly so deleted something\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOVDc7c9xzyY",
        "colab_type": "code",
        "outputId": "459d6afd-9bae-40fe-e9f1-88ec1f297587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "batcher.x.size(), batcher.y.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([265755, 64, 8]), torch.Size([265755, 64]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUo7XsXlx1V-",
        "colab_type": "code",
        "outputId": "d140ef20-90e3-46ee-bfba-113b8b39551b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x, y = batcher.x, batcher.y\n",
        "x[0][4], y[0][4]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0, 1, 2, 3, 5, 6, 7, 8]), tensor(4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ta-KvO85-CJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loader =  DataLoader(batcher, batch_size=BATCH_SIZE, shuffle=True)\n",
        "train, test = torch.utils.data.random_split(batcher, [int(batcher.x.size()[0]*0.8), int(batcher.x.size()[0]*0.2)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4UUisJaCx-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train,  shuffle=True) #batch_size = 1, потому что уже внутри batch.x и batch.y есть разделение на батчи\n",
        "test_loader = DataLoader(test, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6G67in4DJhb",
        "colab_type": "code",
        "outputId": "2bf6684c-0039-418b-852f-a174e0230a51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for x, y in train_loader:\n",
        "  print(x.size(), y.size())\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 64, 8]) torch.Size([1, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8ZfdHeD_QvM",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn94QET2DrRs",
        "colab_type": "text"
      },
      "source": [
        "trained word vectors (mention somewhere, how long it took to train)\n",
        "\n",
        "plotted loss (so we can see that it has converged)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVIXFG6J_S-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super(MyModel, self).__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=batcher.word2index['<pad>'])\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        \n",
        "        self.fc1 = nn.Linear(embed_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
        "        \n",
        "    def forward(self, x):    \n",
        "\n",
        "        x = x.squeeze(0)\n",
        "        print(x.size())\n",
        "        x = self.embedding(x)\n",
        "        print(x.size())\n",
        "        x = self.fc1(x)\n",
        "        print(x.size())\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        print(x.size())\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1hbuPi12E9V",
        "colab_type": "code",
        "outputId": "3326edea-0881-46c3-aff1-c5f8d608aaeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "model = MyModel(vocab_size=batcher.vocab_size, embed_size=100, hidden_size=256)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=batcher.word2index['<pad>'])\n",
        "criterion.cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CrossEntropyLoss()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxZtecvTICtO",
        "colab_type": "code",
        "outputId": "631b4443-6920-4f74-c7b3-906003c26944",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "for x, y in train_loader:\n",
        "  print(model(x).size(), y.squeeze(0).unsqueeze(1).size())\n",
        "  break #посмотрите внимательно на размерность"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 8])\n",
            "torch.Size([64, 8, 100])\n",
            "torch.Size([64, 8, 256])\n",
            "torch.Size([64, 8, 63632])\n",
            "torch.Size([64, 8, 63632]) torch.Size([64, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reMsQfGxHjIz",
        "colab_type": "code",
        "outputId": "a3798357-b683-474b-e6d8-63baf4e7564e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(batcher.word2index), batcher.vocab_size, len(train_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(63632, 63632, 212604)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLsV87u6FMmM",
        "colab_type": "code",
        "outputId": "27b2c29e-903c-46c8-f7a2-7a407d3a02d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890,
          "referenced_widgets": [
            "117125abe5e34fac9ce47eea7de86ccb",
            "6906e05a5bb544fcb3c2fb340d58f831",
            "01f6e53e6f2e405fa8568b91a619bd24",
            "880eca7a029845759da84cd931ec8300",
            "8fc46a39fc5a485fac430715b2194f91",
            "efe25e67c55a49d8b2eacd29f0350727",
            "da0b353a3d4a42098720c0e29235c60a",
            "22c31003a709484eaa1392d3f8bfcfbd"
          ]
        }
      },
      "source": [
        "def train_epoch(data_iter, model, criterion):\n",
        "    total_loss = 0\n",
        "    data_iter = tqdm_notebook(data_iter, total=len(train_loader))\n",
        "    counter = 0\n",
        "    for x,y in train_loader:\n",
        "\n",
        "        y =  y.squeeze(0)\n",
        "        \n",
        "        out = model.forward(x)\n",
        "        print(y.size())\n",
        "        loss = criterion(out, y).item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "\n",
        "        optimizer.step()\n",
        "        #optimizer.zero_grad()\n",
        "        \n",
        "        total_loss += loss\n",
        "        data_iter.set_postfix(loss = loss.item())\n",
        "        counter +=1\n",
        "        \n",
        "    total_loss /= counter\n",
        "    return total_loss\n",
        "\n",
        "def valid_epoch(data_iter, model, criterion):\n",
        "    total_loss = 0\n",
        "    data_iter = tqdm_notebook(data_iter, total=len(test_loader))\n",
        "    counter = 0\n",
        "    for x,y in test_loader:\n",
        "\n",
        "        y = y.squeeze(0)\n",
        "        \n",
        "        out = model.forward(x)\n",
        "        loss = criterion(out, y).item()\n",
        "        \n",
        "        total_loss += loss\n",
        "        data_iter.set_postfix(loss = loss)\n",
        "        counter +=1\n",
        "        \n",
        "    total_loss /= counter\n",
        "    return total_loss\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    loss = train_epoch(train_loader, model, criterion).item()\n",
        "    print('train', loss)\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        loss = valid_epoch(test_loader, model, criterion)\n",
        "        scheduler.step(loss)\n",
        "        print('valid', loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "117125abe5e34fac9ce47eea7de86ccb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=212604), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 8])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-cdf23388c516>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-cdf23388c516>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(data_iter, model, criterion)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-cfff97454408>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Float but got scalar type Long for argument #2 'mat1' in call to _th_addmm"
          ]
        }
      ]
    }
  ]
}